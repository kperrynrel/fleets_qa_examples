{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d68d6d5d",
   "metadata": {},
   "source": [
    "# Running PVAnalytics QA Routines on Temperature Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440479be",
   "metadata": {},
   "source": [
    "Import all of the necessary packages for running QA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "195b044f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import timezonefinder\n",
    "from statistics import mode\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import rdtools\n",
    "import pvanalytics\n",
    "from pvanalytics.quality import data_shifts as ds\n",
    "from pvanalytics.quality import gaps\n",
    "from pvanalytics.quality.outliers import zscore\n",
    "from pvanalytics.features.daytime import power_or_irradiance\n",
    "from pvanalytics.system import is_tracking_envelope\n",
    "import pva_time_shifts as tsh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2275ba5c",
   "metadata": {},
   "source": [
    "## Irradiance Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61302692",
   "metadata": {},
   "source": [
    "The code below can be used to run a variation of the PV Fleets (https://www.nrel.gov/pv/fleet-performance-data-initiative.html) QA routine on a measured temperature stream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f871f029",
   "metadata": {},
   "source": [
    "First, define time series CSV, and associated system latitude-longitude coordinates. Latitude-longitude coordinates will be used to model solar output at the specified location using pvlib functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "572fd6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./data/system_4_module_temperature.csv\"\n",
    "latitude = 39.7406\n",
    "longitude = -105.1774\n",
    "# Identify the temperature data stream type (this affects the type of checks we do)\n",
    "data_stream_type = \"module\"\n",
    "\n",
    "# Read in the file as as pandas series\n",
    "time_series = pd.read_csv(file_path, parse_dates=True, index_col=0).squeeze()\n",
    "\n",
    "# Remove any duplicate index values (this is just a failsafe in case there are!)\n",
    "time_series = time_series.groupby(time_series.index).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49923518",
   "metadata": {},
   "source": [
    "Get the associated data sampling frequency of the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04c3141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007-08-25 17:00:00-07:00    21.58800\n",
      "2007-08-25 17:15:00-07:00    21.97200\n",
      "2007-08-25 17:30:00-07:00    21.92000\n",
      "2007-08-25 17:45:00-07:00    22.01300\n",
      "2007-08-25 18:00:00-07:00    21.41400\n",
      "                               ...   \n",
      "2023-09-29 14:00:00-07:00    15.80904\n",
      "2023-09-29 14:15:00-07:00    14.18414\n",
      "2023-09-29 14:30:00-07:00    13.17503\n",
      "2023-09-29 14:45:00-07:00    12.64178\n",
      "2023-09-29 15:00:00-07:00    12.58730\n",
      "Name: module_temp_1, Length: 564377, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Show the time series just to check it's timezone-aware. This is important for all future analysis, as many\n",
    "# PVAnalytics functions require tz-aware data.\n",
    "\n",
    "print(time_series)\n",
    "\n",
    "# Get the time series frequency\n",
    "freq_minutes = int((mode(abs(np.diff(time_series.index)))).total_seconds() / 60)\n",
    "data_freq = str(freq_minutes) + \"T\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a44efb5",
   "metadata": {},
   "source": [
    "## Visualize Original Time Series\n",
    "\n",
    "Visualize the starting time series as reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d3e6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series.plot(title=\"Original Time Series\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095f6c56",
   "metadata": {},
   "source": [
    "## Run Basic Data Checks: Stale and Abnormal Data\n",
    "\n",
    "Basic data checks include removing the following:\n",
    "1) Flatlined/stale data periods (https://pvanalytics.readthedocs.io/en/stable/generated/gallery/gaps/stale-data.html#sphx-glr-generated-gallery-gaps-stale-data-py)\n",
    "\n",
    "2) \"Abnormal\" data periods, which are out of the temperature limits of -40 to 185 deg C. Additional checks based on thresholds are applied depending on the type of temperature sensor (ambient or module)\n",
    "\n",
    "3) Outliers, which are defined as more than one 4 standard deviations away from the mean (https://pvanalytics.readthedocs.io/en/stable/generated/gallery/outliers/zscore-outlier-detection.html#sphx-glr-generated-gallery-outliers-zscore-outlier-detection-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc0bf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE STALE DATA\n",
    "# Stale data mask\n",
    "stale_data_mask = gaps.stale_values_round(time_series,\n",
    "                                          window=3,\n",
    "                                          decimals=2)\n",
    "\n",
    "# FIND ABNORMAL PERIODS\n",
    "temperature_limit_mask = pvanalytics.quality.weather.temperature_limits(time_series,\n",
    "                                                                        limits=(-40, 185))\n",
    "temperature_limit_mask = temperature_limit_mask.reindex(index=time_series.index,\n",
    "                                                        method='ffill',\n",
    "                                                        fill_value=False)\n",
    "\n",
    "# FIND OUTLIERS (Z-SCORE FILTER)\n",
    "zscore_outlier_mask = zscore(time_series,\n",
    "                             zmax=4,\n",
    "                             nan_policy='omit')\n",
    "\n",
    "# PERFORM ADDITIONAL CHECKS, INCLUDING CHECKING UNITS (CELSIUS OR FAHRENHEIT)\n",
    "temperature_mean = time_series.mean()\n",
    "if temperature_mean > 35:\n",
    "    temp_units = 'F'\n",
    "else:\n",
    "    temp_units = 'C'\n",
    "\n",
    "print(\"Estimated Temperature units: \" + str(temp_units))\n",
    "\n",
    "# Run additional checks based on temperature sensor type.\n",
    "if data_stream_type == 'module':\n",
    "    if temp_units == 'C':\n",
    "        module_limit_mask = (time_series <= 85)\n",
    "        temperature_limit_mask = (temperature_limit_mask & module_limit_mask)\n",
    "if data_stream_type == 'ambient':\n",
    "    ambient_limit_mask = pvanalytics.quality.weather.temperature_limits(\n",
    "            time_series, limits=(-40, 120))\n",
    "    temperature_limit_mask = (temperature_limit_mask & ambient_limit_mask)\n",
    "    if temp_units == 'C':\n",
    "        ambient_limit_mask_2 = (time_series <= 50)\n",
    "        temperature_limit_mask = (temperature_limit_mask & ambient_limit_mask_2)\n",
    "# Get the percentage of data flagged for each issue, so it can later be logged\n",
    "pct_stale = round((len(time_series[stale_data_mask].dropna())/len(time_series.dropna())*100), 1)\n",
    "pct_erroneous = round((len(time_series[~temperature_limit_mask].dropna())/len(time_series.dropna())*100), 1)\n",
    "pct_outlier = round((len(time_series[zscore_outlier_mask].dropna())/len(time_series.dropna())*100), 1)\n",
    "\n",
    "# Visualize all of the time series issues (stale, abnormal, outlier, etc)\n",
    "time_series.plot()\n",
    "labels = [\"Temperature\"]\n",
    "if any(stale_data_mask):\n",
    "    time_series.loc[stale_data_mask].plot(ls='', marker='o', color = \"green\")\n",
    "    labels.append(\"Stale\")\n",
    "if any(~temperature_limit_mask):\n",
    "    time_series.loc[~temperature_limit_mask].plot(ls='', marker='o', color = \"yellow\")\n",
    "    labels.append(\"Abnormal\")\n",
    "if any(zscore_outlier_mask):\n",
    "    time_series.loc[zscore_outlier_mask].plot(ls='', marker='o', color = \"purple\")\n",
    "    labels.append(\"Outlier\")\n",
    "plt.legend(labels=labels)\n",
    "plt.title(\"Time Series Labeled for Basic Issues\")\n",
    "plt.xticks(rotation=20)\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Temperature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Filter the time series, taking out all of the issues\n",
    "time_series = time_series[~stale_data_mask]\n",
    "time_series = time_series[temperature_limit_mask]\n",
    "time_series = time_series[~zscore_outlier_mask]\n",
    "time_series = time_series.asfreq(data_freq)\n",
    "\n",
    "# Visualize the time series post-filtering\n",
    "time_series.plot(title = \"Time Series Post-Basic Data Filtering\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c055f87b",
   "metadata": {},
   "source": [
    "## Filter by Completeness Score\n",
    "Filter the time series by a daily \"completeness\" score (https://pvanalytics.readthedocs.io/en/stable/generated/gallery/gaps/data-completeness.html#sphx-glr-generated-gallery-gaps-data-completeness-py).This filtering scheme requires at least 25% of data to be present for each day for it to be included. We further require that there be at least 10 consecutive days meeting this 25% threshold for the day to be included (see https://pvanalytics.readthedocs.io/en/stable/generated/pvanalytics.quality.gaps.trim_incomplete.html#pvanalytics.quality.gaps.trim_incomplete for more info)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b592633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize daily data completeness\n",
    "data_completeness_score = gaps.completeness_score(time_series)\n",
    "\n",
    "# Visualize data completeness score as a time series.\n",
    "data_completeness_score.plot()\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Daily Completeness Score (Fractional)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Trim the series based on daily completeness score\n",
    "trim_series = pvanalytics.quality.gaps.trim_incomplete(time_series,\n",
    "                                                       minimum_completeness=.25,\n",
    "                                                       freq=data_freq)\n",
    "first_valid_date, last_valid_date = pvanalytics.quality.gaps.start_stop_dates(trim_series)\n",
    "time_series = time_series[first_valid_date.tz_convert(time_series.index.tz):last_valid_date.tz_convert(time_series.index.tz)]\n",
    "time_series = time_series.asfreq(data_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51ec6ca",
   "metadata": {},
   "source": [
    "## Determine Data Shifts\n",
    "\n",
    "Examine the daily time series for abrupt capacity shifts, and determine the longest continuous part of the series free of data shifts. Once again, changepoint detection is used to determine shift dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a2dd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the time series to daily mean\n",
    "time_series_daily = time_series.resample('D').mean()\n",
    "data_shift_start_date, data_shift_end_date = \\\n",
    "            ds.get_longest_shift_segment_dates(time_series_daily)\n",
    "data_shift_period_length = (data_shift_end_date - data_shift_start_date).days\n",
    "\n",
    "# Get the number of shift dates\n",
    "data_shift_mask = pvanalytics.quality.data_shifts.detect_data_shifts(time_series_daily)\n",
    "# Get the shift dates\n",
    "shift_dates = list(time_series_daily[data_shift_mask].index)\n",
    "if len(shift_dates) > 0:            \n",
    "    shift_found = True\n",
    "else:\n",
    "    shift_found = False\n",
    "\n",
    "# Visualize the time shifts for the daily time series\n",
    "print(\"Shift Found??\")\n",
    "print(shift_found)\n",
    "edges = [time_series_daily.index[0]] + shift_dates + [time_series_daily.index[-1]]\n",
    "fig, ax = plt.subplots()\n",
    "for (st, ed) in zip(edges[:-1], edges[1:]):\n",
    "    ax.plot(time_series_daily.loc[st:ed])\n",
    "plt.title(\"Daily Time Series Labeled for Data Shifts\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9f4b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the time series to only include the longest shift-free period\n",
    "time_series = time_series[(time_series.index >= data_shift_start_date.tz_convert(time_series.index.tz)) &\n",
    "                          (time_series.index <= data_shift_end_date.tz_convert(time_series.index.tz))]\n",
    "\n",
    "time_series = time_series.asfreq(data_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647e5f33",
   "metadata": {},
   "source": [
    "## Display the final filtered time series\n",
    "\n",
    "Show the final time series post-processing as reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e5b850",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series.plot(title=\"Final Filtered Time Series\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0bae86",
   "metadata": {},
   "source": [
    "## Generate QA Report\n",
    "\n",
    "Generate a dictionary outlining all of the different issues present for the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b141f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_check_dict = {\"pct_stale\": pct_stale,\n",
    "                 \"pct_negative\": pct_negative,\n",
    "                 \"pct_erroneous\": pct_erroneous,\n",
    "                 \"pct_outlier\": pct_outlier,\n",
    "                 \"data_shifts\": shift_found,\n",
    "                 \"shift_dates\": shift_dates,\n",
    "                 \"filtered_time_series\": time_series}\n",
    "\n",
    "print(\"QA Results:\")\n",
    "print(qa_check_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
